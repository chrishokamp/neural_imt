import numpy

import numpy
from fuel.datasets import TextFile
from fuel.schemes import ConstantScheme
from fuel.transformers import (
    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping, Transformer, SourcewiseTransformer)

from six.moves import cPickle

from machine_translation.stream import (_ensure_special_tokens, _length, PaddingWithEOS, _oov_to_unk, _too_long,
                                        ShuffleBatchTransformer)



def map_pair_to_imt_triples(source, reference, bos_token=None, eos_token=None):
    """
    Map a (source, reference) pair into (len(actual_reference) + 2) new examples

    Assumes that users always want the empty prefix at the beginning of the generated examples
    (i.e. ask the system for the full hypothesis) and the empty suffix (i.e. ask the system for nothing)
    at the end of the generated examples

    By passing None for bos_token or eos_token, user indicates that these tokens have already
    been prepended or appended to the reference

    Note: may want to refactor this into a function which takes just the target and returns (prefix, suffix) pairs
    for more flexibility

    """

    start_index = 0
    if bos_token:
        start_index = 1

    end_index = len(reference) + 1
    if eos_token:
        end_index -= 1

    prefixes, suffixes = zip(*[(reference[:i], reference[i:]) for i in range(start_index, end_index)])
    sources = [source for _ in range(end_index - start_index)]

    assert len(sources) == len(prefixes) == len(suffixes), 'All components must have the same length'

    return zip(sources, prefixes, suffixes)


# Module for functionality associated with streaming data
class PrefixSuffixStreamTransformer:
    """
    Takes a stream of (source, target) and adds the sources ('suffixes', 'prefixes'),



    Parameters
    ----------


    Notes
    -----
    At call time, we expect a stream providing (sources, references) -- i.e. something like a TextFile object

    In the future, we may want to randomly provide _some_ of the prefix/suffix pairs for a target sequence, but not
    all of them, this would require initializing the transformer with some params.

    Note that we also need to duplicate the source and target the required number of times (this depends upon
    the length of the target sequence). Currently this is accomplished in a separate transformer
    """

    def __init__(self, **kwargs):
        pass

    def __call__(self, data, **kwargs):
        source = data[0]
        reference = data[1]

        # TODO: we need to pass through the information about BOS and EOS tokens
        # TODO: there is wasted computation here, since we will need to flatten the sources back out again later
        sources, target_prefixes, target_suffixes = zip(*map_pair_to_imt_triples(source, reference,
                                                                                 bos_token=True,
                                                                                 eos_token=True,
                                                                                 **kwargs))

        # Note: the cast here is important, otherwise these will become float64s which will break everything
        target_prefixes = [numpy.array(pre).astype('int64') for pre in target_prefixes]
        target_suffixes = [numpy.array(suf).astype('int64') for suf in target_suffixes]

        return (target_prefixes, target_suffixes)


# Module for functionality associated with streaming data
class IMTSampleStreamTransformer:
    """
    Stateful transformer which takes a stream of (source, target) and adds the sources ('samples', 'scores')

    Samples are generated by calling the sample func with the source as argument

    Scores are generated by comparing each generated sample to the reference

    Parameters
    ----------
    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples
    score_func: function

    At call time, we expect a stream providing (sources, references) -- i.e. something like a TextFile object


    """

    def __init__(self, sample_func, score_func, num_samples=1, **kwargs):
        self.sample_func = sample_func
        self.score_func = score_func
        self.num_samples = num_samples
        # kwargs will get passed to self.score_func when it gets called
        self.kwargs = kwargs

    def __call__(self, data, **kwargs):
        source = data[0]
        reference = data[1]
        prefix = data[2]
        suffix = data[3]

        # each sample may be of different length
        samples = self.sample_func(numpy.array(source), numpy.array(prefix), self.num_samples)
        # TODO: we currently have to pass the source because of the interface to mteval_v13
        scores = numpy.array(self._compute_scores(source, reference, samples, **self.kwargs)).astype('float32')

        return (samples, scores)

    # Note that many sentence-level metrics like BLEU can be computed directly over the indexes (not the strings),
    # Note that some sentence-level metrics like METEOR require the string representation
    # if the scoring function needs to map from ints to strings, provide 'src_vocab' and 'trg_vocab' via the kwargs
    # So we don't need to map back to a string representation
    def _compute_scores(self, source, reference, samples, **kwargs):
        """Call the scoring function to compare each sample to the reference"""

        return self.score_func(source, reference, samples, **kwargs)


class CopySourceAndTargetToMatchPrefixes(Transformer):
    """Duplicate the source and target to match the number of prefixes and suffixes

    We need this transformer because Fuel does not directly support transformers which _BOTH_ add sources _and_
    modify existing sources

    Parameters
    ----------
    data_stream : :class:`AbstractDataStream` instance
        The data stream to wrap

    Notes
    -----

    It isn't very nice how all of the source names are hard-coded

    """

    @property
    def sources(self):
        # data_stream should be an attribute of the parent class
        return self.data_stream.sources

    def __call__(self, data, **kwargs):
        batch_obj = {k:v for k,v in zip(self.data_stream.sources, data)}
        num_prefixes = len(batch_obj['target_prefix'])

        batch_obj['source'] = tuple([batch_obj['source'] for _ in range(num_prefixes)])
        batch_obj['target'] = tuple([batch_obj['target'] for _ in range(num_prefixes)])

        batch_with_expanded_source_and_target = [batch_obj[k] for k in self.data_stream.sources]

        return tuple(batch_with_expanded_source_and_target)


def get_tr_stream_with_prefixes(src_vocab, trg_vocab, src_data, trg_data, src_vocab_size=30000,
                                trg_vocab_size=30000, unk_id=1, seq_len=50,
                                batch_size=80, sort_k_batches=12, **kwargs):
    """Prepares the IMT training data stream."""

    # Load dictionaries and ensure special tokens exist
    src_vocab = _ensure_special_tokens(
        src_vocab if isinstance(src_vocab, dict)
        else cPickle.load(open(src_vocab)),
        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)
    trg_vocab = _ensure_special_tokens(
        trg_vocab if isinstance(trg_vocab, dict) else
        cPickle.load(open(trg_vocab)),
        bos_idx=0, eos_idx=trg_vocab_size - 1, unk_idx=unk_id)

    # Get text files from both source and target
    src_dataset = TextFile([src_data], src_vocab,
                           bos_token='<S>',
                           eos_token='</S>',
                           unk_token='<UNK>')
    trg_dataset = TextFile([trg_data], trg_vocab,
                           bos_token='<S>',
                           eos_token='</S>',
                           unk_token='<UNK>')

    # Merge them to get a source, target pair
    stream = Merge([src_dataset.get_example_stream(),
                    trg_dataset.get_example_stream()],
                   ('source', 'target'))

    # Filter sequences that are too long
    stream = Filter(stream,
                    predicate=_too_long(seq_len=seq_len))

    # Replace out of vocabulary tokens with unk token
    # TODO: doesn't the TextFile stream do this anyway?
    stream = Mapping(stream,
                     _oov_to_unk(src_vocab_size=src_vocab_size,
                                 trg_vocab_size=trg_vocab_size,
                                 unk_id=unk_id))

    stream = Mapping(stream, PrefixSuffixStreamTransformer(),
                     add_sources=('target_prefix', 'target_suffix'))

    stream = Mapping(stream, CopySourceAndTargetToMatchPrefixes(stream))

    # changing stream.produces_examples is a little hack which lets us use Unpack to flatten
    stream.produces_examples = False
    # flatten the stream back out into (source, target, target_prefix, target_suffix)
    stream = Unpack(stream)

    # Now make a very big batch that we can shuffle
    # Build a batched version of stream to read k batches ahead
    shuffle_batch_size = kwargs['shuffle_batch_size']
    stream = Batch(stream,
                   iteration_scheme=ConstantScheme(shuffle_batch_size)
                   )

    stream = ShuffleBatchTransformer(stream)

    # unpack it again
    stream = Unpack(stream)

    # Build a batched version of stream to read k batches ahead
    stream = Batch(stream,
                   iteration_scheme=ConstantScheme(batch_size * sort_k_batches)
                   )

    # Sort all samples in the read-ahead batch
    stream = Mapping(stream, SortMapping(_length))

    # Convert it into a stream again
    stream = Unpack(stream)

    # Construct batches from the stream with specified batch size
    stream = Batch(
        stream, iteration_scheme=ConstantScheme(batch_size))

    # Pad sequences that are short
    # TODO: is it correct to blindly pad the target_prefix and the target_suffix?
    masked_stream = PaddingWithEOS(
        stream, [src_vocab_size - 1, trg_vocab_size - 1, trg_vocab_size - 1, trg_vocab_size - 1],
        mask_sources=('source', 'target', 'target_prefix', 'target_suffix'))

    return masked_stream, src_vocab, trg_vocab


# # Remember that the BleuValidator does hackish stuff to get target set information from the main_loop data_stream
# # using all kwargs here makes it more clear that this function is always called with get_dev_stream(**config_dict)
def get_dev_stream_with_prefixes(val_set=None, val_set_grndtruth=None, src_vocab=None, src_vocab_size=30000,
                                 trg_vocab=None, trg_vocab_size=30000, unk_id=1, return_vocab=False, **kwargs):
    """Setup development set stream if necessary."""

    dev_stream = None
    if val_set is not None and val_set_grndtruth is not None:
        src_vocab = _ensure_special_tokens(
            src_vocab if isinstance(src_vocab, dict) else
            cPickle.load(open(src_vocab)),
            bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)

        trg_vocab = _ensure_special_tokens(
            trg_vocab if isinstance(trg_vocab, dict) else
            cPickle.load(open(trg_vocab)),
            bos_idx=0, eos_idx=trg_vocab_size - 1, unk_idx=unk_id)

        dev_source_dataset = TextFile([val_set], src_vocab,
                                      bos_token='<S>',
                                      eos_token='</S>',
                                      unk_token='<UNK>')
        dev_target_dataset = TextFile([val_set_grndtruth], trg_vocab,
                                      bos_token='<S>',
                                      eos_token='</S>',
                                      unk_token='<UNK>')

        dev_stream = Merge([dev_source_dataset.get_example_stream(),
                            dev_target_dataset.get_example_stream()],
                           ('source', 'target'))

        # now add prefix and suffixes to this stream
        dev_stream = Mapping(dev_stream, PrefixSuffixStreamTransformer(),
                         add_sources=('target_prefix', 'target_suffix'))

        dev_stream = Mapping(dev_stream, CopySourceAndTargetToMatchPrefixes(dev_stream))

        # changing stream.produces_examples is a little hack which lets us use Unpack to flatten
        dev_stream.produces_examples = False
        # flatten the stream back out into (source, target, target_prefix, target_suffix)
        dev_stream = Unpack(dev_stream)

    if return_vocab:
        return dev_stream, src_vocab, trg_vocab
    else:
        return dev_stream


class CopySourceAndPrefixNTimes(Transformer):
    """Duplicate the source N times to match the number of samples

    We need this transformer because the attention model expects one source sequence for each
    target sequence, but in the sampling case there are effectively (instances*sample_size) target sequences

    Parameters
    ----------
    data_stream : :class:`AbstractDataStream` instance
        The data stream to wrap
    n_samples : int -- the number of samples that were generated for each source sequence

    """
    def __init__(self, data_stream, n_samples=5, **kwargs):
        if data_stream.produces_examples:
            raise ValueError('the wrapped data stream must produce batches of '
                             'examples, not examples')
        self.n_samples = n_samples

        super(CopySourceAndPrefixNTimes, self).__init__(
            data_stream, produces_examples=False, **kwargs)


    @property
    def sources(self):
        return self.data_stream.sources

    def transform_batch(self, batch):
        batch_with_expanded_source = []
        for i, (source, source_batch) in enumerate(
                zip(self.data_stream.sources, batch)):
            if source == 'source' or source == 'target_prefix':

                # copy each source sequence self.n_samples times, but keep the tensor 2d
                expanded_source = []
                for ins in source_batch:
                    expanded_source.extend([ins for _ in range(self.n_samples)])

                batch_with_expanded_source.append(expanded_source)
            else:
                batch_with_expanded_source.append(source_batch)

        return tuple(batch_with_expanded_source)

