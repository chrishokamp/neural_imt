{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Fuel stream which takes pairs of (source, target)\n",
    "# and converts them into (len(source_sentence) + 1) triples of (source, prefix, completion),\n",
    "#      at training time, take the (source, prefix) inputs, and sample N\n",
    "# system should also be able to propose just the 'EOS </S>' token -- if the target prefix looks like \n",
    "# the correct hypothesis, do nothing\n",
    "\n",
    "# include direct machine translation as the '0' output -- given the <S> token,\n",
    "# the system first presents a full hypothesis\n",
    "\n",
    "# TODO: make sure <S> and </S> are correctly handled in beam search and evaluation\n",
    "\n",
    "# WORKING: new evaluation metric, implemented as \n",
    "# `from machine_translation.evaluation import sentence_level_bleu`\n",
    "\n",
    "# to evaluate the model on new data, we need three parallel files (sources, prefixes, reference_completions)\n",
    "# when writing the output of the model, we output N completions per input, and separate the N outputs for each\n",
    "# input by a blank line -- similar to WMT QE feature format\n",
    "\n",
    "# for each line, compute the score, then average all of them to get the score for the dataset\n",
    "\n",
    "# notes on baselines\n",
    "# we should evaluate both F1 as in Ueffing and Ney, and NCDG for ranking\n",
    "#     - plain F1 is the primary metric, since this fits a wider range of usecases\n",
    "# the easiest baseline is to ignore the target prefix completely, and just generate a full hypothesis for the source\n",
    "# - if the target prefix matches the prefix of the hypothesis, compute f1 as usual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imt_f1(hyp, ref):\n",
    "    \"\"\"\n",
    "    compute Ueffing and Ney F1 for IMT\n",
    "    \n",
    "    Note that this function is agnostic about its inputs, as long as they\n",
    "    are sequences. Thus the metric can be computed for sequences of characters, \n",
    "    words, phrases, etc...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # if both are empty, this is a perfect match\n",
    "    if len(hyp) == 0 and len(ref) == 0:\n",
    "        return 1.\n",
    "    \n",
    "    match_len = float(0)\n",
    "    hyp_len = float(len(hyp))\n",
    "    ref_len = float(len(ref))\n",
    "    for h_sym, r_sym in zip(hyp, ref):\n",
    "        if h_sym == r_sym:\n",
    "            match_len += 1.\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if match_len == 0:\n",
    "        return 0.\n",
    "    \n",
    "    # ratio of characters in the prediction which are correct (low if prefix is too long)\n",
    "    precision = match_len / hyp_len        \n",
    "        \n",
    "    # ratio of coverage of the reference (low if prefix is too short)\n",
    "    recall = match_len / ref_len\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imt_f1('a sand', 'a sandwich made of gold')\n",
    "# imt_f1('fish sandwich', 'a sandwich')\n",
    "\n",
    "imt_f1('', '')\n",
    "imt_f1('a sand', 'a sandwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from theano import tensor\n",
    "\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "from fuel.transformers import Transformer\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens, MTSampleStreamTransformer,\n",
    "                                        get_textfile_stream, _too_long, _length, PaddingWithEOS,\n",
    "                                        _oov_to_unk)\n",
    "\n",
    "\n",
    "from machine_translation.evaluation import sentence_level_bleu\n",
    "\n",
    "try:\n",
    "    from blocks_extras.extensions.plot import Plot\n",
    "    BOKEH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BOKEH_AVAILABLE = False\n",
    "\n",
    "\n",
    "# build the training and sampling graphs for minimum risk training\n",
    "# Intialize the MTSampleStreamTransformer with the sampling function\n",
    "\n",
    "# load a model that's already trained, and start tuning it with minimum-risk\n",
    "# mock-up training using the blocks main loop\n",
    "\n",
    "# TODO: Integrate configuration so min-risk training is a single line in the config file\n",
    "# TODO: this requires handling both the data stream and the Model cost function\n",
    "\n",
    "# create the graph which can sample from our model\n",
    "# Note that we must sample instead of getting the 1-best or N-best, because we need the randomness to make the expected\n",
    "# BLEU score make sense\n",
    "\n",
    "BASEDIR = '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/' +\\ \n",
    "              'BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "              '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/'\n",
    "#best_bleu_model_1455464992_BLEU31.61.npz\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'saved_parameters': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/' +\\\n",
    "        'BERTHA-TEST_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000' +\\ \n",
    "        '_emb300_rec800_batch15/best_bleu_model_1455410311_BLEU30.38.npz',\n",
    "    'src_vocab': os.path.join(BASEDIR, 'vocab.en-de.en.pkl'),\n",
    "    'trg_vocab': os.path.join(BASEDIR, 'vocab.en-de.de.pkl'),\n",
    "    'src_data': os.path.join(BASEDIR, 'training_data/train.en.tok.shuf'),\n",
    "    'trg_data': os.path.join(BASEDIR, 'training_data/train.de.tok.shuf'),\n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': os.path.join(os.path.dirname(os.path.realpath(__file__)),\n",
    "                                '../test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl'),\n",
    "\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 8,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 2,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    # Beam-size\n",
    "    'beam_size': 10,\n",
    "\n",
    "    # Maximum number of updates\n",
    "    'finish_after': 1000000,\n",
    "\n",
    "    # Reload model from files if exist\n",
    "    'reload': False,\n",
    "\n",
    "    # Save model after this many updates\n",
    "    'save_freq': 500,\n",
    "\n",
    "    # Show samples from model after this many updates\n",
    "    'sampling_freq': 1000,\n",
    "\n",
    "    # Show this many samples at each sampling\n",
    "    'hook_samples': 5,\n",
    "\n",
    "    # Validate bleu after this many updates\n",
    "    'bleu_val_freq': 10,\n",
    "    # Normalize cost according to sequence length after beam-search\n",
    "    'normalized_bleu': True,\n",
    "    \n",
    "    'saveto': '/media/1tb_drive/test_min_risk_model_save',\n",
    "    'model_save_directory': 'test_min_risk_model_save',\n",
    "    # Validation set source file\n",
    "    'val_set': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.en.tok',\n",
    "\n",
    "    # Validation set gold file\n",
    "    'val_set_grndtruth': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.de.tok',\n",
    "\n",
    "    # Print validation output to file\n",
    "    'output_val_set': True,\n",
    "\n",
    "    # Validation output file\n",
    "    'val_set_out': '/media/1tb_drive/test_min_risk_model_save/validation_out.txt',\n",
    "    'val_burn_in': 0,\n",
    "\n",
    "    # NEW PARAM FOR MIN RISK TRAINING\n",
    "    'n_samples': 100\n",
    "\n",
    "}\n",
    "\n",
    "# this returns the function for sampling from the model\n",
    "def get_sampling_model_and_input(exp_config):\n",
    "    # Create Theano variables\n",
    "    encoder = BidirectionalEncoder(\n",
    "        exp_config['src_vocab_size'], exp_config['enc_embed'], exp_config['enc_nhids'])\n",
    "\n",
    "#   TODO: decoder with special generate function which can take a prefix as input\n",
    "    decoder = Decoder(\n",
    "        exp_config['trg_vocab_size'], exp_config['dec_embed'], exp_config['dec_nhids'],\n",
    "        exp_config['enc_nhids'] * 2, loss_function='min_risk')\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    sampling_input = tensor.lmatrix('source')\n",
    "\n",
    "    # TODO: beam search which can take (source, prefix) as input\n",
    "    # Get beam search\n",
    "    logger.info(\"Building sampling model\")\n",
    "    sampling_representation = encoder.apply(\n",
    "        sampling_input, tensor.ones(sampling_input.shape))\n",
    "    generated = decoder.generate(sampling_input, sampling_representation)\n",
    "\n",
    "#     _, samples = VariableFilter(\n",
    "#         bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                  ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "#     beam_search = BeamSearch(samples=samples)\n",
    "\n",
    "    # build the model that will let us get a theano function from the sampling graph\n",
    "    logger.info(\"Creating Sampling Model...\")\n",
    "    sampling_model = Model(generated)\n",
    "\n",
    "    return sampling_model, sampling_input, encoder, decoder\n",
    "\n",
    "sample_model, theano_sampling_input, train_encoder, train_decoder = get_sampling_model_and_input(exp_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test that we can pull samples from the model\n",
    "trg_vocab = cPickle.load(open(exp_config['trg_vocab']))\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "src_vocab = cPickle.load(open(exp_config['src_vocab']))\n",
    "src_vocab_size = exp_config['src_vocab_size'] - 1\n",
    "\n",
    "src_vocab = _ensure_special_tokens(src_vocab, bos_idx=0,\n",
    "                                   eos_idx=src_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "trg_vocab = _ensure_special_tokens(trg_vocab, bos_idx=0,\n",
    "                                   eos_idx=trg_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "theano_sample_func = sample_model.get_theano_function()\n",
    "\n",
    "# note: we close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "# TODO: or by avoiding the for loop somehow\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "\n",
    "    def _get_true_length(seqs, vocab):\n",
    "        try:\n",
    "            lens = []\n",
    "            for r in seqs.tolist():\n",
    "                lens.append(r.index(vocab['</S>']) + 1)\n",
    "            return lens\n",
    "        except ValueError:\n",
    "            return [seqs.shape[1] for _ in range(seqs.shape[0])]\n",
    "\n",
    "    # samples = []\n",
    "    # for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        # _1, outputs, _2, _3, costs = theano_sample_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        # outputs = outputs.reshape(outputs.shape[0])\n",
    "        # outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        # samples.append(outputs)\n",
    "\n",
    "    inputs = numpy.tile(source_seq[None, :], (num_samples, 1))\n",
    "    # the output is [seq_len, batch]\n",
    "    _1, outputs, _2, _3, costs = theano_sample_func(inputs)\n",
    "    outputs = outputs.T\n",
    "\n",
    "    # TODO: this step could be avoided by computing the samples mask in a different way\n",
    "    lens = _get_true_length(outputs, trg_vocab)\n",
    "    samples = [s[:l] for s,l in zip(outputs.tolist(), lens)]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])\n",
    "\n",
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))\n",
    "\n",
    "# Filter sequences that are too long\n",
    "training_stream = Filter(training_stream,\n",
    "                         predicate=_too_long(seq_len=exp_config['seq_len']))\n",
    "\n",
    "# sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, sentence_level_bleu, num_samples=exp_config['n_samples'])\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))\n",
    "\n",
    "# TODO: this method connects the data stream with the decoder cost function, is there a better way to decouple?\n",
    "class FlattenSamples(Transformer):\n",
    "    \"\"\"Flatten one dimension from the samples, the reshaping for cost computation\n",
    "    is done inside the cost function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        super(FlattenSamples, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "#         if mask_dtype is None:\n",
    "#             self.mask_dtype = config.floatX\n",
    "#         else:\n",
    "#             self.mask_dtype = mask_dtype\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "#         sources = []\n",
    "#         for source in self.data_stream.sources:\n",
    "#             sources.append(source)\n",
    "#             if source in self.mask_sources:\n",
    "#                 sources.append(source + '_mask')\n",
    "#         return tuple(sources)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_flattened_samples = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "#             if source not in self.mask_sources:\n",
    "#                 batch_with_masks.append(source_batch)\n",
    "#                 continue\n",
    "            if source == 'samples':\n",
    "                flattened_samples = []\n",
    "                for ins in source_batch:\n",
    "                    for sample in ins:\n",
    "                        flattened_samples.append(sample)\n",
    "                batch_with_flattened_samples.append(flattened_samples)\n",
    "            else:\n",
    "                batch_with_flattened_samples.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_flattened_samples)\n",
    "\n",
    "# TODO: modify this transformer to split a (source, target) stream into (source, prefix, completion) triples\n",
    "class CopySourceNTimes(Transformer):\n",
    "    \"\"\"Duplicate the source N times to match the number of samples\n",
    "\n",
    "    We need this transformer because the attention model expects one source sequence for each\n",
    "    target sequence, but in the sampling case there are effectively (instances*sample_size) target sequences\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "    n_samples : int -- the number of samples that were generated for each source sequence\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, n_samples=5, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        super(CopySourceNTimes, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_expanded_source = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "            if source == 'source':\n",
    "#                 copy each source seqoyuence self.n_samples times, but keep the tensor 2d\n",
    "\n",
    "                expanded_source = []\n",
    "                for ins in source_batch:\n",
    "                    expanded_source.extend([ins for _ in range(self.n_samples)])\n",
    "\n",
    "                batch_with_expanded_source.append(expanded_source)\n",
    "            else:\n",
    "                batch_with_expanded_source.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_expanded_source)\n",
    "\n",
    "\n",
    "\n",
    "# Replace out of vocabulary tokens with unk token\n",
    "# training_stream = Mapping(training_stream,\n",
    "#                  _oov_to_unk(src_vocab_size=exp_config['src_vocab_size'],\n",
    "#                              trg_vocab_size=exp_config['trg_vocab_size'],\n",
    "#                              unk_id=exp_config['unk_id']))\n",
    "\n",
    "# Build a batched version of stream to read k batches ahead\n",
    "training_stream = Batch(training_stream,\n",
    "               iteration_scheme=ConstantScheme(\n",
    "                   exp_config['batch_size']*exp_config['sort_k_batches']))\n",
    "\n",
    "# Sort all samples in the read-ahead batch\n",
    "training_stream = Mapping(training_stream, SortMapping(_length))\n",
    "\n",
    "# Convert it into a stream again\n",
    "training_stream = Unpack(training_stream)\n",
    "\n",
    "# Construct batches from the stream with specified batch size\n",
    "training_stream = Batch(\n",
    "    training_stream, iteration_scheme=ConstantScheme(exp_config['batch_size']))\n",
    "\n",
    "# Pad sequences that are short\n",
    "# IDEA: add a transformer which flattens the target samples before we add the mask\n",
    "flat_sample_stream = FlattenSamples(training_stream)\n",
    "\n",
    "expanded_source_stream = CopySourceNTimes(flat_sample_stream, n_samples=exp_config['n_samples'])\n",
    "\n",
    "# TODO: some sources can be excluded from the padding Op, but since blocks matches sources with input variable\n",
    "# TODO: names, it's not critical\n",
    "masked_stream = PaddingWithEOS(\n",
    "    expanded_source_stream, [exp_config['src_vocab_size'] - 1, exp_config['trg_vocab_size'] - 1])\n",
    "\n",
    "\n",
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "\n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "    samples = tensor.lmatrix('samples')\n",
    "    samples_mask = tensor.matrix('samples_mask')\n",
    "\n",
    "    # scores is (batch, samples)\n",
    "    scores = tensor.matrix('scores')\n",
    "    # We don't need a scores mask because there should be the same number of scores for each instance\n",
    "    # num samples is a hyperparameter of the model\n",
    "\n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "#     decoder.name = 'decoder'\n",
    "\n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = decoder.expected_cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def main(model, cost, config, tr_stream, dev_stream, use_bokeh=False):\n",
    "\n",
    "    # Set the parameters from a trained models (.npz file)\n",
    "    logger.info(\"Loading parameters from model: {}\".format(exp_config['saved_parameters']))\n",
    "    # Note the brick delimeter='-' is here for legacy reasons because blocks changed the serialization API\n",
    "    param_values = LoadNMT.load_parameter_values(config['saved_parameters'], brick_delimiter='-')\n",
    "    LoadNMT.set_model_parameters(model, param_values)\n",
    "\n",
    "    logger.info('Creating computational graph')\n",
    "    cg = ComputationGraph(cost)\n",
    "\n",
    "    # create the training directory, and copy this config there if directory doesn't exist\n",
    "#     if not os.path.isdir(config['saveto']):\n",
    "#         os.makedirs(config['saveto'])\n",
    "#         shutil.copy(config['config_file'], config['saveto'])\n",
    "\n",
    "    # Set extensions\n",
    "    logger.info(\"Initializing extensions\")\n",
    "    extensions = [\n",
    "        FinishAfter(after_n_batches=config['finish_after']),\n",
    "        TrainingDataMonitoring([cost], after_batch=True),\n",
    "        Printing(after_batch=True),\n",
    "         CheckpointNMT(config['saveto'],\n",
    "                       every_n_batches=config['save_freq'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Set up beam search and sampling computation graphs if necessary\n",
    "\n",
    "    if config['hook_samples'] >= 1 or config['bleu_script'] is not None:\n",
    "        logger.info(\"Building sampling model\")\n",
    "        sampling_representation = train_encoder.apply(\n",
    "            theano_sampling_input, tensor.ones(theano_sampling_input.shape))\n",
    "        # TODO: the generated output actually contains several more values, ipdb to see what they are\n",
    "        generated = train_decoder.generate(theano_sampling_input, sampling_representation)\n",
    "        search_model = Model(generated)\n",
    "        _, samples = VariableFilter(\n",
    "            bricks=[train_decoder.sequence_generator], name=\"outputs\")(\n",
    "                ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "\n",
    "    # Add sampling\n",
    "    if config['hook_samples'] >= 1:\n",
    "        logger.info(\"Building sampler\")\n",
    "        extensions.append(\n",
    "            Sampler(model=search_model, data_stream=tr_stream,\n",
    "                    hook_samples=config['hook_samples'],\n",
    "                    every_n_batches=config['sampling_freq'],\n",
    "                    src_vocab_size=config['src_vocab_size']))\n",
    "\n",
    "    # Add early stopping based on bleu\n",
    "    if config['bleu_script'] is not None:\n",
    "        logger.info(\"Building bleu validator\")\n",
    "        extensions.append(\n",
    "            BleuValidator(theano_sampling_input, samples=samples, config=config,\n",
    "                          model=search_model, data_stream=dev_stream,\n",
    "                          normalize=config['normalized_bleu'],\n",
    "                          every_n_batches=config['bleu_val_freq']))\n",
    "\n",
    "    # Reload model if necessary\n",
    "    if config['reload']:\n",
    "        extensions.append(LoadNMT(config['saveto']))\n",
    "\n",
    "    # Plot cost in bokeh if necessary\n",
    "    if use_bokeh and BOKEH_AVAILABLE:\n",
    "        extensions.append(\n",
    "            Plot(config['model_save_directory'], channels=[['decoder_cost_cost'], ['validation_set_bleu_score']],\n",
    "                 every_n_batches=10))\n",
    "\n",
    "    # Set up training algorithm\n",
    "    logger.info(\"Initializing training algorithm\")\n",
    "    # if there is dropout or random noise, we need to use the output of the modified graph\n",
    "#     if config['dropout'] < 1.0 or config['weight_noise_ff'] > 0.0:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cg.outputs[0], parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "#     else:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cost, parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                 eval(config['step_rule'])()],\n",
    "                               ),\n",
    "        on_unused_sources='warn'\n",
    "    )\n",
    "\n",
    "    # enrich the logged information\n",
    "    extensions.append(\n",
    "        Timing(every_n_batches=100)\n",
    "    )\n",
    "\n",
    "    # Initialize main loop\n",
    "    logger.info(\"Initializing main loop\")\n",
    "    main_loop = MainLoop(\n",
    "        model=model,\n",
    "        algorithm=algorithm,\n",
    "        data_stream=tr_stream,\n",
    "        extensions=extensions\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    main_loop.run()\n",
    "\n",
    "\n",
    "training_cost = create_model(train_encoder, train_decoder)\n",
    "\n",
    "\n",
    "# Set up training model\n",
    "logger.info(\"Building model\")\n",
    "train_model = Model(training_cost)\n",
    "\n",
    "\n",
    "# test_iter = masked_stream.get_epoch_iterator()\n",
    "\n",
    "# source, source_mask, target, target_mask, samples, samples_mask, scores, scores_mask = test_iter.next()\n",
    "\n",
    "# train_model.inputs\n",
    "\n",
    "# test_func = train_model.get_theano_function()\n",
    "\n",
    "# scores = scores.astype('float32')\n",
    "# out = test_func(scores, samples_mask, source_mask, source, samples)\n",
    "\n",
    "\n",
    "# numpy.exp(out)\n",
    "# out[0].shape\n",
    "# out\n",
    "# scores\n",
    "# src_ivocab = {v:k for k,v in src_vocab.items()}\n",
    "\n",
    "dev_stream = get_dev_stream(**exp_config)\n",
    "\n",
    "main(train_model, training_cost, exp_config, masked_stream, dev_stream=dev_stream, use_bokeh=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
