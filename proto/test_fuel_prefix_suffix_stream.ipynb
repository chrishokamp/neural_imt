{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample config for test experiments\n",
    "BASEDIR='/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_Adam_wmt-multimodal_internal_data_'+\\\n",
    "    'dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/'\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "#     'saved_parameters': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "#     '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "    'src_vocab': os.path.join(BASEDIR, 'vocab.en-de.en.pkl'),\n",
    "    'trg_vocab': os.path.join(BASEDIR, 'vocab.en-de.de.pkl'),\n",
    "    'src_data': os.path.join(BASEDIR, 'training_data/train.en.tok.shuf'),\n",
    "    'trg_data': os.path.join(BASEDIR, 'training_data/train.de.tok.shuf'),\n",
    "    \n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': '/home/chris/projects/neural_mt/test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl',\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 5,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 1,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    'finish_after': 100,\n",
    "    \n",
    "    # WORKING: add config for evaluation with IMT model -- follow NMT evaluation config\n",
    "\n",
    "    ##############                                                                                  \n",
    "    # PREDICTION #                                                                                              \n",
    "    ##############      \n",
    "    'prefix_decoding': True,\n",
    "    'brick_delimiter': '-',\n",
    "    'beam_size': 10,\n",
    "    'normalized_bleu': True,\n",
    "\n",
    "    # Note we _do not_ need the target_prefixes, since these are generated on the fly\n",
    "\n",
    "    'source_lang': 'en',                                                                                                    \n",
    "    'target_lang': 'de',                                                                                                                 \n",
    "\n",
    "    'n_best': 1,                                                                                                                                 \n",
    "\n",
    "    # path to the moses perl script for tokenization                                                                                    \n",
    "    'tokenize_script': None,                                                                                                                                \n",
    "    # path to the moses perl script for detokenization                                                                                              \n",
    "    'detokenize_script': None,                                                                                                                              \n",
    "\n",
    "    # The location of the saved parameters of a trained model as .npz                                                                       \n",
    "    # TODO: model save directory is currently misnamed -- switch to yaml configs with good model names                          \n",
    "    'saved_parameters': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_Adam_wmt-multimodal_internal_data_'+\\\n",
    "    'dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/'+\\\n",
    "    'best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "                                                                                                                                                           \n",
    "                                                                                                                                                        \n",
    "    # The location of a test set in the source language                                                                                                             \n",
    "    'test_set': '/media/1tb_drive/multilingual-multimodal/flickr30k/test/test.en.tok',                                   \n",
    "    #'test_set': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.en.tok'                                                        \n",
    "                                                                                                                            \n",
    "    # your desired path to the translated output file, or an already-translated file that you just want to evaluate                                           \n",
    "    'translated_output_file': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/test.imt_PROTO-DELETE-ME.30.x.hyps.out',         \n",
    "    #'translated_output_file': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.multimodal-summed.30.x.hyps.out'             \n",
    "                                                                                                                                                            \n",
    "    # The location of the gold standard references for the test set (for evaluation mode only)                                                  \n",
    "    'test_gold_refs': '/media/1tb_drive/multilingual-multimodal/flickr30k/test/test.de.tok', \n",
    "    \n",
    "    # if the config contains this key, meteor will also be computed                                                                                                          â”‚\n",
    "    'meteor_directory': '/home/chris/programs/meteor-1.5'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "# Test a fuel stream which maps (source, target) into (source, target, target_prefix, target_suffix)\n",
    "\n",
    "from nn_imt.stream import (PrefixSuffixStreamTransformer, CopySourceAndTargetToMatchPrefixes)\n",
    "\n",
    "from machine_translation.stream import ShuffleBatchTransformer\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from fuel.datasets import IterableDataset\n",
    "from fuel.transformers import Merge\n",
    "from fuel.streams import DataStream\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "import numpy\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping, Transformer)\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "from machine_translation.stream import _ensure_special_tokens, _length, PaddingWithEOS, _oov_to_unk, _too_long\n",
    "\n",
    "# Working: modify this for IMT\n",
    "def get_tr_stream_with_prefixes(src_vocab, trg_vocab, src_data, trg_data, src_vocab_size=30000,\n",
    "                                trg_vocab_size=30000, unk_id=1, seq_len=50,\n",
    "                                batch_size=80, sort_k_batches=12, **kwargs):\n",
    "    \"\"\"Prepares the training data stream.\"\"\"\n",
    "\n",
    "    # Load dictionaries and ensure special tokens exist\n",
    "    src_vocab = _ensure_special_tokens(\n",
    "        src_vocab if isinstance(src_vocab, dict)\n",
    "        else cPickle.load(open(src_vocab)),\n",
    "        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "    trg_vocab = _ensure_special_tokens(\n",
    "        trg_vocab if isinstance(trg_vocab, dict) else\n",
    "        cPickle.load(open(trg_vocab)),\n",
    "        bos_idx=0, eos_idx=trg_vocab_size - 1, unk_idx=unk_id)\n",
    "\n",
    "    # Get text files from both source and target\n",
    "    src_dataset = TextFile([src_data], src_vocab, None)\n",
    "    trg_dataset = TextFile([trg_data], trg_vocab, None)\n",
    "\n",
    "    # Merge them to get a source, target pair\n",
    "    stream = Merge([src_dataset.get_example_stream(),\n",
    "                    trg_dataset.get_example_stream()],\n",
    "                   ('source', 'target'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream,\n",
    "                    predicate=_too_long(seq_len=seq_len))\n",
    "\n",
    "    # Replace out of vocabulary tokens with unk token\n",
    "    # TODO: doesn't the TextFile stream do this anyway?\n",
    "    stream = Mapping(stream,\n",
    "                     _oov_to_unk(src_vocab_size=src_vocab_size,\n",
    "                                 trg_vocab_size=trg_vocab_size,\n",
    "                                 unk_id=unk_id))\n",
    "\n",
    "    # \n",
    "    stream = Mapping(stream, PrefixSuffixStreamTransformer(),\n",
    "                     add_sources=('target_prefix', 'target_suffix'))\n",
    "    \n",
    "    # WORKING: add transformer to duplicate the source and target len(target_prefix) times\n",
    "    stream = Mapping(stream, CopySourceAndTargetToMatchPrefixes(stream))\n",
    "    \n",
    "    # changing stream.produces_examples is a little hack which lets us use Unpack to flatten\n",
    "    stream.produces_examples = False\n",
    "    # flatten the stream back out into (source, target, target_prefix, target_suffix)\n",
    "    stream = Unpack(stream)\n",
    "    \n",
    "    # Now make a very big batch that we can shuffle\n",
    "    # TODO: let user configure the size of this shuffle batch\n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "    SHUFFLE_BATCH_SIZE = 1000\n",
    "    stream = Batch(stream,\n",
    "                   iteration_scheme=ConstantScheme(SHUFFLE_BATCH_SIZE)\n",
    "                  )\n",
    "    \n",
    "    # WORKING: implement shuffled batch transformer in nnmt\n",
    "    stream = ShuffleBatchTransformer(stream)\n",
    "    \n",
    "    # unpack it again\n",
    "    stream = Unpack(stream)\n",
    "\n",
    "\n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "    stream = Batch(stream,\n",
    "                   iteration_scheme=ConstantScheme(batch_size*sort_k_batches)\n",
    "                  )\n",
    "\n",
    "    # Sort all samples in the read-ahead batch\n",
    "    stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "    stream = Unpack(stream)\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    print(src_vocab_size)\n",
    "    # Pad sequences that are short\n",
    "    # TODO: is it correct to blindly pad the target_prefix and the target_suffix?\n",
    "    masked_stream = PaddingWithEOS(\n",
    "        stream, [src_vocab_size - 1, trg_vocab_size - 1, trg_vocab_size - 1, trg_vocab_size - 1],\n",
    "        mask_sources=('source', 'target', 'target_prefix', 'target_suffix'))\n",
    "\n",
    "    return masked_stream, src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "test_tr_stream, src_vocab, trg_vocab = get_tr_stream_with_prefixes(**exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = test_tr_stream.get_epoch_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source,source_mask,target,target_mask,target_prefix,target_prefix_mask,target_suffix,target_suffix_mask = t.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trg_ivocab = {v:k for k,v in trg_vocab.items() if k != '<S>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    3    10     6 19999 19999 19999 19999 19999 19999 19999 19999 19999\n",
      " 19999 19999 19999]\n",
      "prefix: Ein Mann , </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n",
      "suffix: der im Wasser paddelt . </S> </S> </S>\n",
      "[    3   760    22   465  6378     6  5475    18  1284 19999 19999 19999\n",
      " 19999 19999 19999]\n",
      "prefix: Ein Team von vier Richtern , bewertet einen Wettbewerb </S> </S> </S> </S> </S> </S>\n",
      "suffix: . </S> </S> </S> </S> </S> </S> </S>\n",
      "[    3    10     5 19999 19999 19999 19999 19999 19999 19999 19999 19999\n",
      " 19999 19999 19999]\n",
      "prefix: Ein Mann in </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n",
      "suffix: einem grauen Oberteil jongliert mit Kegeln . </S>\n",
      "[   12   277    15     8    11  8553  3494  2145     7 19999 19999 19999\n",
      " 19999 19999 19999]\n",
      "prefix: Eine blonde Frau mit einer 40er Jahre Frisur und </S> </S> </S> </S> </S> </S>\n",
      "suffix: Leopardenmuster Hemd . </S> </S> </S> </S> </S>\n",
      "[   3   67   25    8  202    9    4 5513  444   14   34    8 3037  357    2]\n",
      "prefix: Ein kleines MÃ¤dchen mit Helm auf einem Zweirad nahe der StraÃŸe mit fahrenden Autos .\n",
      "suffix: </S> </S> </S> </S> </S> </S> </S> </S>\n"
     ]
    }
   ],
   "source": [
    "for pre, suf in zip(target_prefix, target_suffix):\n",
    "    print(pre)\n",
    "    print('prefix: {}'.format(' '.join([trg_ivocab[int(w)] for w in pre])))\n",
    "    print('suffix: {}'.format(' '.join([trg_ivocab[int(w)] for w in suf])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('source',\n",
       " 'source_mask',\n",
       " 'target',\n",
       " 'target_mask',\n",
       " 'target_prefix',\n",
       " 'target_prefix_mask',\n",
       " 'target_suffix',\n",
       " 'target_suffix_mask')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tr_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
